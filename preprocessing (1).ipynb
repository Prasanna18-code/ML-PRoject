{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55960288",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os, joblib\n",
    "\n",
    "print(\"\\n==================== CLEAN + PREPROCESS PIPELINE START ====================\\n\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. LOAD RAW DATA\n",
    "# ---------------------------------------------------------\n",
    "file_path = \"/Users/praveenkumardevamane/Downloads/Dynamic pricing/DynamicPricing2/data/retail_ecommerce_dataset_10k.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Raw data loaded. Shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a8ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. BASIC CLEANING\n",
    "# ---------------------------------------------------------\n",
    "# Convert numeric safely\n",
    "for col in [\"Current Price\", \"Competitor Price\", \"Historical Price\",\n",
    "            \"Inventory Level\", \"Purchase Quantity\", \"Total Revenue\",\n",
    "            \"Competitor Popularity\", \"Price Elasticity\"]:\n",
    "    if col in data.columns:\n",
    "        data[col] = pd.to_numeric(data[col], errors=\"coerce\")\n",
    "\n",
    "# Convert timestamp\n",
    "data[\"Transaction Timestamp\"] = pd.to_datetime(data[\"Transaction Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Fill numeric NaN with median\n",
    "num_cols_basic = data.select_dtypes(include=[\"number\"]).columns\n",
    "for col in num_cols_basic:\n",
    "    if data[col].isnull().any():\n",
    "        data[col] = data[col].fillna(data[col].median())\n",
    "\n",
    "# Drop rows where timestamp missing\n",
    "before = len(data)\n",
    "data = data.dropna(subset=[\"Transaction Timestamp\"])\n",
    "print(f\"Dropped {before - len(data)} rows with missing timestamp.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f24d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 3. STANDARDIZE CATEGORY TEXT\n",
    "# ---------------------------------------------------------\n",
    "def clean_text(x):\n",
    "    if pd.isna(x): return \"Unknown\"\n",
    "    return str(x).strip().replace(\" \", \"_\")\n",
    "\n",
    "for col in [\"Product Name\", \"Product Category\", \"Customer Demographics\", \"Customer Region\"]:\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e5003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 4. SORT FOR CORRECT ROLLINGS\n",
    "# ---------------------------------------------------------\n",
    "data = data.sort_values(by=[\"Product ID\", \"Transaction Timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7d6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 5. FEATURE ENGINEERING (NO LAGS)\n",
    "# ---------------------------------------------------------\n",
    "# Rolling price features\n",
    "data[\"rolling_avg_7_day\"] = data.groupby(\"Product ID\")[\"Current Price\"].transform(lambda x: x.rolling(7).mean())\n",
    "data[\"rolling_avg_30_day\"] = data.groupby(\"Product ID\")[\"Current Price\"].transform(lambda x: x.rolling(30).mean())\n",
    "data[\"rolling_volatility_7_day\"] = data.groupby(\"Product ID\")[\"Current Price\"].transform(lambda x: x.rolling(7).std())\n",
    "\n",
    "# Competitor price features\n",
    "data[\"competitor_price_difference\"] = data[\"Current Price\"] - data[\"Competitor Price\"]\n",
    "data[\"competitor_price_ratio\"] = data[\"Current Price\"] / data[\"Competitor Price\"]\n",
    "\n",
    "# Discount % handling\n",
    "data[\"discount_percentage\"] = (\n",
    "    data[\"Discount Applied\"].astype(str).str.replace(\"%\", \"\", regex=False)\n",
    ").astype(float) / 100\n",
    "\n",
    "# Price momentum (NO lag)\n",
    "data[\"price_change_rate\"] = data.groupby(\"Product ID\")[\"Current Price\"].pct_change().replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# Fill engineered NaNs with median\n",
    "eng_cols = [\n",
    "    \"rolling_avg_7_day\", \"rolling_avg_30_day\", \"rolling_volatility_7_day\",\n",
    "    \"competitor_price_difference\", \"competitor_price_ratio\",\n",
    "    \"discount_percentage\", \"price_change_rate\"\n",
    "]\n",
    "\n",
    "for col in eng_cols:\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].fillna(data[col].median())\n",
    "\n",
    "print(\"Feature engineering completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60033ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 6. REMOVE LEAKAGE FEATURES\n",
    "# ---------------------------------------------------------\n",
    "leakage_cols = [\"Historical Price\", \"Total Revenue\"]\n",
    "data = data.drop(columns=[c for c in leakage_cols if c in data.columns], errors=\"ignore\")\n",
    "print(\"Removed leakage columns:\", leakage_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f1d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 7. DEFINE TARGET AND FEATURES\n",
    "# ---------------------------------------------------------\n",
    "y = data[\"Current Price\"]\n",
    "X = data.drop(columns=[\n",
    "    \"Transaction ID\", \"Transaction Timestamp\", \"Current Price\",\n",
    "    \"Discount Applied\", \"Product ID\"\n",
    "], errors=\"ignore\")\n",
    "\n",
    "print(\"Final feature set shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab100e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 8. SPLIT TRAIN / VAL / TEST\n",
    "# ---------------------------------------------------------\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f203ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 9. IDENTIFY TYPES\n",
    "# ---------------------------------------------------------\n",
    "num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(\"\\nNumeric cols:\", len(num_cols))\n",
    "print(\"Categorical cols:\", len(cat_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d6bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(\"\\nRunning SAFE imputers...\")\n",
    "\n",
    "# ============= NUMERIC IMPUTATION =============\n",
    "num_cols_existing = [c for c in num_cols if c in X_train.columns]\n",
    "\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "train_num = num_imputer.fit_transform(X_train[num_cols_existing])\n",
    "val_num   = num_imputer.transform(X_val[num_cols_existing])\n",
    "test_num  = num_imputer.transform(X_test[num_cols_existing])\n",
    "\n",
    "# Convert back to DataFrame to avoid mismatches\n",
    "train_num_df = pd.DataFrame(train_num, columns=num_cols_existing, index=X_train.index)\n",
    "val_num_df   = pd.DataFrame(val_num,   columns=num_cols_existing, index=X_val.index)\n",
    "test_num_df  = pd.DataFrame(test_num,  columns=num_cols_existing, index=X_test.index)\n",
    "\n",
    "# Assign safely\n",
    "for col in num_cols_existing:\n",
    "    X_train[col] = train_num_df[col]\n",
    "    X_val[col]   = val_num_df[col]\n",
    "    X_test[col]  = test_num_df[col]\n",
    "\n",
    "\n",
    "# ============= CATEGORICAL IMPUTATION =============\n",
    "cat_cols_existing = [c for c in cat_cols if c in X_train.columns]\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "train_cat = cat_imputer.fit_transform(X_train[cat_cols_existing])\n",
    "val_cat   = cat_imputer.transform(X_val[cat_cols_existing])\n",
    "test_cat  = cat_imputer.transform(X_test[cat_cols_existing])\n",
    "\n",
    "train_cat_df = pd.DataFrame(train_cat, columns=cat_cols_existing, index=X_train.index)\n",
    "val_cat_df   = pd.DataFrame(val_cat,   columns=cat_cols_existing, index=X_val.index)\n",
    "test_cat_df  = pd.DataFrame(test_cat,  columns=cat_cols_existing, index=X_test.index)\n",
    "\n",
    "for col in cat_cols_existing:\n",
    "    X_train[col] = train_cat_df[col]\n",
    "    X_val[col]   = val_cat_df[col]\n",
    "    X_test[col]  = test_cat_df[col]\n",
    "\n",
    "print(\"SAFE imputation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e97d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFixing numerical columns for scaling...\")\n",
    "\n",
    "for col in numerical_cols:\n",
    "    # Fill NaN values with median\n",
    "    data[col] = data[col].fillna(data[col].median())\n",
    "\n",
    "    # If column has zero variance (std = 0), add tiny noise so scaler works\n",
    "    if data[col].std() == 0:\n",
    "        data[col] += 1e-9\n",
    "        print(f\"   Fixed zero-variance column: {col}\")\n",
    "\n",
    "print(\"Numerical columns ready for scaling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d69dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 9. SCALE NUMERICAL FEATURES\n",
    "# ---------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train[numerical_cols]), columns=numerical_cols, index=X_train.index)\n",
    "X_val_scaled   = pd.DataFrame(scaler.transform(X_val[numerical_cols]), columns=numerical_cols, index=X_val.index)\n",
    "X_test_scaled  = pd.DataFrame(scaler.transform(X_test[numerical_cols]), columns=numerical_cols, index=X_test.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 10. ENCODE CATEGORICAL FEATURES\n",
    "# ---------------------------------------------------------\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "X_train_encoded = pd.DataFrame(encoder.fit_transform(X_train[categorical_cols]), index=X_train.index)\n",
    "X_val_encoded   = pd.DataFrame(encoder.transform(X_val[categorical_cols]), index=X_val.index)\n",
    "X_test_encoded  = pd.DataFrame(encoder.transform(X_test[categorical_cols]), index=X_test.index)\n",
    "\n",
    "# Add column names\n",
    "X_train_encoded.columns = encoder.get_feature_names_out(categorical_cols)\n",
    "X_val_encoded.columns   = encoder.get_feature_names_out(categorical_cols)\n",
    "X_test_encoded.columns  = encoder.get_feature_names_out(categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# 11. MERGE NUMERIC + ENCODED CATEGORICAL\n",
    "# ---------------------------------------------------------\n",
    "X_train_processed = pd.concat([X_train_scaled, X_train_encoded], axis=1)\n",
    "X_val_processed   = pd.concat([X_val_scaled, X_val_encoded], axis=1)\n",
    "X_test_processed  = pd.concat([X_test_scaled, X_test_encoded], axis=1)\n",
    "\n",
    "print(\"Final processed train shape:\", X_train_processed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de3d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 12. SAVE PROCESSED DATA + OBJECTS\n",
    "# ---------------------------------------------------------\n",
    "os.makedirs(\"preprocessed_data\", exist_ok=True)\n",
    "\n",
    "X_train_processed.to_csv(\"preprocessed_data/X_train_processed.csv\", index=False)\n",
    "X_val_processed.to_csv(\"preprocessed_data/X_val_processed.csv\", index=False)\n",
    "X_test_processed.to_csv(\"preprocessed_data/X_test_processed.csv\", index=False)\n",
    "\n",
    "y_train.to_csv(\"preprocessed_data/y_train.csv\", index=False)\n",
    "y_val.to_csv(\"preprocessed_data/y_val.csv\", index=False)\n",
    "y_test.to_csv(\"preprocessed_data/y_test.csv\", index=False)\n",
    "\n",
    "joblib.dump(scaler, \"preprocessed_data/scaler.pkl\")\n",
    "joblib.dump(encoder, \"preprocessed_data/encoder.pkl\")\n",
    "\n",
    "with open(\"preprocessed_data/numerical_cols.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(numerical_cols))\n",
    "\n",
    "with open(\"preprocessed_data/categorical_cols.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(categorical_cols))\n",
    "\n",
    "print(\"\\n==================== PREPROCESSING PIPELINE COMPLETE ====================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a0672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "489ea8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== PREPROCESSING PIPELINE START ====================\n",
      "\n",
      "Loaded raw data: (10000, 20)\n",
      "After timestamp cleaning: (10000, 20)\n",
      "Feature engineering complete (NO rolling features).\n",
      "Removed leakage features: ['Historical Price', 'Total Revenue']\n",
      "Feature matrix shape: (10000, 17)\n",
      "Train: (7000, 17) | Val: (1500, 17) | Test: (1500, 17)\n",
      "Numeric columns: ['Competitor Price', 'Inventory Level', 'Promotion Status', 'Purchase Quantity', 'Competitor Popularity', 'Seasonal Indicator', 'Price Elasticity', 'Customer Loyalty', 'competitor_price_difference', 'competitor_price_ratio', 'discount_percentage', 'price_change_rate']\n",
      "Categorical columns: ['Product Name', 'Product Category', 'Customer Demographics', 'Customer Region', 'Market Segment']\n",
      "Numeric imputation complete.\n",
      "Categorical imputation complete.\n",
      "Processed shapes: (7000, 107) (1500, 107) (1500, 107)\n",
      "\n",
      "==================== PREPROCESSING COMPLETE ====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================= PREPROCESSING PIPELINE (NO ROLLING FEATURES) =========================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(\"\\n==================== PREPROCESSING PIPELINE START ====================\\n\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. LOAD RAW DATA\n",
    "# -------------------------------------------------------------------------\n",
    "file_path = \"/Users/praveenkumardevamane/Downloads/Dynamic pricing/DynamicPricing2/data/retail_ecommerce_dataset_10k.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "print(\"Loaded raw data:\", data.shape)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. BASIC TYPE FIXING\n",
    "# -------------------------------------------------------------------------\n",
    "data[\"Current Price\"] = pd.to_numeric(data[\"Current Price\"], errors=\"coerce\")\n",
    "data[\"Competitor Price\"] = pd.to_numeric(data[\"Competitor Price\"], errors=\"coerce\")\n",
    "data[\"Transaction Timestamp\"] = pd.to_datetime(data[\"Transaction Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Fill minimal NaNs\n",
    "for col in [\"Current Price\", \"Competitor Price\"]:\n",
    "    if data[col].isnull().any():\n",
    "        data[col] = data[col].fillna(data[col].median())\n",
    "\n",
    "# Drop rows without timestamps\n",
    "data = data.dropna(subset=[\"Transaction Timestamp\"])\n",
    "print(\"After timestamp cleaning:\", data.shape)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. SORT BY PRODUCT + TIME (Required for consistency)\n",
    "# -------------------------------------------------------------------------\n",
    "data = data.sort_values(by=[\"Product ID\", \"Transaction Timestamp\"])\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4. FEATURE ENGINEERING (NO ROLLING FEATURES)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Competitor features\n",
    "data[\"competitor_price_difference\"] = data[\"Current Price\"] - data[\"Competitor Price\"]\n",
    "data[\"competitor_price_ratio\"] = data[\"Current Price\"] / data[\"Competitor Price\"]\n",
    "\n",
    "# Discount %\n",
    "data[\"discount_percentage\"] = (\n",
    "    data[\"Discount Applied\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"%\", \"\", regex=False)\n",
    "    .astype(float)\n",
    "    / 100\n",
    ").fillna(0)\n",
    "\n",
    "# Price elasticity proxy â€” safe & simple\n",
    "data[\"price_change_rate\"] = (\n",
    "    data.groupby(\"Product ID\")[\"Current Price\"].diff().fillna(0)\n",
    ")\n",
    "\n",
    "print(\"Feature engineering complete (NO rolling features).\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. REMOVE LEAKAGE FEATURES\n",
    "# -------------------------------------------------------------------------\n",
    "leakage_cols = [\"Historical Price\", \"Total Revenue\"]\n",
    "\n",
    "for col in leakage_cols:\n",
    "    if col in data.columns:\n",
    "        data = data.drop(columns=col)\n",
    "\n",
    "print(\"Removed leakage features:\", leakage_cols)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6. DEFINE X AND y\n",
    "# -------------------------------------------------------------------------\n",
    "y = data[\"Current Price\"]\n",
    "\n",
    "drop_cols = [\"Transaction ID\", \"Transaction Timestamp\", \"Current Price\", \"Discount Applied\", \"Product ID\"]\n",
    "X = data.drop(columns=[c for c in drop_cols if c in data.columns])\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 7. TRAIN / VAL / TEST SPLIT\n",
    "# -------------------------------------------------------------------------\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"| Val:\", X_val.shape, \"| Test:\", X_test.shape)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 8. IDENTIFY COLUMN TYPES\n",
    "# -------------------------------------------------------------------------\n",
    "num_cols = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 9. IMPUTE NUMERIC COLUMNS SAFELY\n",
    "# -------------------------------------------------------------------------\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "X_train[num_cols] = num_imputer.fit_transform(X_train[num_cols])\n",
    "X_val[num_cols]   = num_imputer.transform(X_val[num_cols])\n",
    "X_test[num_cols]  = num_imputer.transform(X_test[num_cols])\n",
    "\n",
    "print(\"Numeric imputation complete.\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 10. IMPUTE CATEGORICAL COLUMNS SAFELY\n",
    "# -------------------------------------------------------------------------\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "X_train[cat_cols] = cat_imputer.fit_transform(X_train[cat_cols])\n",
    "X_val[cat_cols]   = cat_imputer.transform(X_val[cat_cols])\n",
    "X_test[cat_cols]  = cat_imputer.transform(X_test[cat_cols])\n",
    "\n",
    "print(\"Categorical imputation complete.\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 11. SCALE NUMERIC FEATURES\n",
    "# -------------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols, index=X_train.index)\n",
    "X_val_scaled   = pd.DataFrame(scaler.transform(X_val[num_cols]), columns=num_cols, index=X_val.index)\n",
    "X_test_scaled  = pd.DataFrame(scaler.transform(X_test[num_cols]), columns=num_cols, index=X_test.index)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 12. ONE-HOT ENCODE CATEGORICAL FEATURES\n",
    "# -------------------------------------------------------------------------\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "train_cat = encoder.fit_transform(X_train[cat_cols])\n",
    "val_cat   = encoder.transform(X_val[cat_cols])\n",
    "test_cat  = encoder.transform(X_test[cat_cols])\n",
    "\n",
    "train_cat_df = pd.DataFrame(train_cat, columns=encoder.get_feature_names_out(cat_cols), index=X_train.index)\n",
    "val_cat_df   = pd.DataFrame(val_cat,   columns=encoder.get_feature_names_out(cat_cols), index=X_val.index)\n",
    "test_cat_df  = pd.DataFrame(test_cat,  columns=encoder.get_feature_names_out(cat_cols), index=X_test.index)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 13. MERGE FEATURES\n",
    "# -------------------------------------------------------------------------\n",
    "X_train_processed = pd.concat([X_train_scaled, train_cat_df], axis=1)\n",
    "X_val_processed   = pd.concat([X_val_scaled,   val_cat_df],   axis=1)\n",
    "X_test_processed  = pd.concat([X_test_scaled,  test_cat_df],  axis=1)\n",
    "\n",
    "print(\"Processed shapes:\",\n",
    "      X_train_processed.shape,\n",
    "      X_val_processed.shape,\n",
    "      X_test_processed.shape)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 14. SAVE EVERYTHING\n",
    "# -------------------------------------------------------------------------\n",
    "os.makedirs(\"preprocessed_data\", exist_ok=True)\n",
    "\n",
    "X_train_processed.to_csv(\"preprocessed_data/X_train_processed.csv\", index=False)\n",
    "X_val_processed.to_csv(\"preprocessed_data/X_val_processed.csv\", index=False)\n",
    "X_test_processed.to_csv(\"preprocessed_data/X_test_processed.csv\", index=False)\n",
    "\n",
    "y_train.to_csv(\"preprocessed_data/y_train.csv\", index=False)\n",
    "y_val.to_csv(\"preprocessed_data/y_val.csv\", index=False)\n",
    "y_test.to_csv(\"preprocessed_data/y_test.csv\", index=False)\n",
    "\n",
    "joblib.dump(scaler, \"preprocessed_data/scaler.pkl\")\n",
    "joblib.dump(encoder, \"preprocessed_data/encoder.pkl\")\n",
    "joblib.dump(num_imputer, \"preprocessed_data/num_imputer.pkl\")\n",
    "joblib.dump(cat_imputer, \"preprocessed_data/cat_imputer.pkl\")\n",
    "\n",
    "with open(\"preprocessed_data/numerical_cols.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(num_cols))\n",
    "\n",
    "with open(\"preprocessed_data/categorical_cols.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(cat_cols))\n",
    "\n",
    "print(\"\\n==================== PREPROCESSING COMPLETE ====================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ff3b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
